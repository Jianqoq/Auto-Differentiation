{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37cb3cf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T22:24:54.822245Z",
     "start_time": "2023-03-07T22:24:53.577040Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from layers2 import MatMul, softmax\n",
    "from model import SimpleCbow, CBow\n",
    "\n",
    "def print_result(current, total, begin, end):\n",
    "    lis = ['[' if i == 0 else ']' if i == 21 else ' ' for i in range(22)]\n",
    "    index = int(current/total*20)\n",
    "    percentage = format(current*100 / total, '.2f')\n",
    "    if 0 <= index < 20:\n",
    "        pass\n",
    "    else:\n",
    "        index = 20\n",
    "    if index > 0:\n",
    "        for i in range(1,index+1):\n",
    "            lis[i] = u'\\\\u25A0'\n",
    "        string = ''.join(lis)\n",
    "        time = end-  begin\n",
    "        print(f'\\\\r{string} {percentage}% Time: {time:.3f}s', end='', flush=True)\n",
    "    else:\n",
    "        string = ''.join(lis)\n",
    "        time = end-  begin\n",
    "        print(f'\\\\r{string} {percentage}% Time: {time:.3f}s', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8175d0c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T22:44:49.252455Z",
     "start_time": "2023-03-07T22:44:49.202866Z"
    },
    "code_folding": [
     0,
     27,
     86
    ]
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6,4) (2,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3564\\1591194349.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;31m# trainer.plot()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;31m# cooccurrence_matrix = np.unpackbits(np.load('cooccurrence_matrix3.npy'), axis = 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3564\\1591194349.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mbatch_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miters\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miters\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mbatch_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miters\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miters\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;31m#                 print(loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m#                 model.backward()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\NLP_model-main\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, context, target)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mnegative_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"uint8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSigmoid_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mnegative_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding_dot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnegative_sample_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding_dot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSigmoid_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSigmoid_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnegative_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\NLP_model-main\\model.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mnegative_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"uint8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSigmoid_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mnegative_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding_dot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnegative_sample_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding_dot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSigmoid_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSigmoid_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnegative_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\NLP_model-main\\layers2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, h, index)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6,4) (2,4) "
     ]
    }
   ],
   "source": [
    "def remove_duplicate(params, grads):\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j] \n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self,x,t,max_epoch=10,batch_size=32,max_grad=None,eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # シャッフル\n",
    "            idx = np.random.permutation(np.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters * batch_size:(iters + 1) * batch_size]\n",
    "                batch_t = t[iters * batch_size:(iters + 1) * batch_size]\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "#                 print(loss)\n",
    "#                 model.backward()\n",
    "#                 params, grads = remove_duplicate(model.params,\n",
    "#                                                  model.grads)  # 共有された重みを1つに集約\n",
    "#                 if max_grad is not None:\n",
    "#                     clip_grads(grads, max_grad)\n",
    "#                 optimizer.update(params, grads)\n",
    "#                 total_loss += loss\n",
    "#                 loss_count += 1\n",
    "\n",
    "#                 # 評価\n",
    "#                 if (eval_interval\n",
    "#                         is not None) and (iters % eval_interval) == 0:\n",
    "#                     avg_loss = total_loss / loss_count\n",
    "#                     elapsed_time = time.time() - start_time\n",
    "#                     print(\n",
    "#                         '\\r| epoch %-5d |  iter %-5d / %-5d | time %10d[s] | loss %.2f'\n",
    "#                         % (self.current_epoch + 1, iters + 1, max_iters,\n",
    "#                            elapsed_time, avg_loss),\n",
    "#                         end='',\n",
    "#                         flush=True)\n",
    "#                     self.loss_list.append(float(avg_loss))\n",
    "#                     total_loss, loss_count = 0, 0\n",
    "\n",
    "#             self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = np.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (\n",
    "            1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "\n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, text: str, *args):\n",
    "        dictionary = {i: f' {i}' for i in args}\n",
    "        text = text.lower()\n",
    "        for i in dictionary:\n",
    "            text = text.replace(i, dictionary.get(i))\n",
    "        self.text = text.split(' ')\n",
    "        self.repeated = []\n",
    "        \n",
    "    def get_word_id(self):\n",
    "        dictionary = {}\n",
    "        dictionary2 = {}\n",
    "        corpus = []\n",
    "        append = corpus.append\n",
    "        counter = 0\n",
    "        for index, i in enumerate(self.text):\n",
    "            if i not in dictionary:\n",
    "                dictionary[i] = counter\n",
    "                dictionary2[counter] = i\n",
    "                counter += 1\n",
    "                append(dictionary[i])\n",
    "            else:\n",
    "                append(dictionary[i])\n",
    "                self.repeated.append(index)\n",
    "        return dictionary, dictionary2, corpus\n",
    "\n",
    "    def get_single_context(self,id_word:dict, word_id:dict, corpus: list, word: str,window: int):  # list bound check\n",
    "        text = self.text\n",
    "        word = word.lower()\n",
    "        length = len(text)\n",
    "        if word not in text:\n",
    "            return\n",
    "        ls = [0] * len(corpus)\n",
    "        for index, i in enumerate(text):\n",
    "            if word_id[i] == word_id[word]:    \n",
    "                if index == 0:\n",
    "                    counter = 1\n",
    "                    for k in range(window):\n",
    "                        ls[counter] += 1\n",
    "                        counter += 1\n",
    "                elif index == length - 1:\n",
    "                    counter = 1\n",
    "                    for p in range(window):\n",
    "                        ls[-1-counter] += 1\n",
    "                        counter += 1\n",
    "                else:\n",
    "                    counter = counter2 = 1\n",
    "                    word1_id = word_id[text[index - counter]]\n",
    "                    word2_id = word_id[text[index + counter2]]\n",
    "                    for p in range(window):\n",
    "                        ls[word1_id] += 1\n",
    "                        ls[word2_id] += 1\n",
    "                        counter += 1\n",
    "                        counter2 += 1\n",
    "                        \n",
    "        return np.array(ls, dtype = 'uint8')\n",
    "\n",
    "    def get_coocurrenceMatrix(self,corpus: list,id_word: dict, word_id: dict, window:int):\n",
    "        ls = []\n",
    "        append = ls.append\n",
    "        total = len(word_id)\n",
    "        begin = time()\n",
    "        for index, i in enumerate(word_id):\n",
    "            append(self.get_single_context(id_word, word_id, corpus, i, window))\n",
    "            print_result(index+1, total, begin, time())\n",
    "        return np.array(ls, dtype = 'uint8'), ls\n",
    "    \n",
    "    def create_context_target(self, corpus, windowsize = 1):\n",
    "        target = corpus[1 : -1]\n",
    "        context = []\n",
    "        cs = []\n",
    "        cs_append = cs.append\n",
    "        context_append = context.append\n",
    "        for i in range(windowsize, len(corpus)-1):\n",
    "            cs.append(corpus[i-1])\n",
    "            cs.append(corpus[i+1])\n",
    "            context.append(cs)\n",
    "            cs=[]\n",
    "        return np.array(context), np.array(target)\n",
    "    \n",
    "    def convert_onehot(self, context, target, length):\n",
    "        zero_context = np.zeros(shape=(*context.shape, length), dtype = 'uint8')\n",
    "        zero_target = np.zeros(shape=(*target.shape, length), dtype = 'uint8')\n",
    "        for index, i in enumerate(context):\n",
    "            for index2, k in enumerate(i):\n",
    "                zero_context[index, index2, k] = 1\n",
    "        for index, i in enumerate(target):\n",
    "                zero_target[index, i] = 1\n",
    "        return zero_context, zero_target\n",
    "    \n",
    "    def PPMI(self, co_matrix, corpus, verbose=True):\n",
    "        ppmi_matrix = np.zeros_like(co_matrix, dtype=np.float32)\n",
    "        N = np.sum(co_matrix)\n",
    "        sigle_word = np.sum(co_matrix, axis = 0)\n",
    "        total = co_matrix.shape[0]*co_matrix.shape[1]\n",
    "        cols = co_matrix.shape[1]\n",
    "        cnt = 0\n",
    "        begin = time()\n",
    "        for i in range(co_matrix.shape[0]):\n",
    "            for j in range(co_matrix.shape[1]):\n",
    "                ppmi = np.log2(co_matrix[i,j]*N/(sigle_word[i]*sigle_word[j]) + 1e-8)\n",
    "                ppmi_matrix[i,j] = max(0, ppmi)\n",
    "                if verbose:\n",
    "                    cnt += 1\n",
    "                    if cnt % (total//200) == 0:\n",
    "                        print_result(cnt+1,total, begin, time())\n",
    "        return ppmi_matrix\n",
    "    def most_similar(self, matrix:list, word:str,word_id:dict, top:int):\n",
    "        word = word.lower()\n",
    "        if word not in word_id:\n",
    "            return\n",
    "        word_use_vector = matrix[word_id[word]]\n",
    "        ls = {id_word[index]:similarity(word_use_vector, i) for index, i in enumerate(matrix) if index is not word_id[word]}\n",
    "        return sorted(ls.items(),key=lambda x:x[1],reverse=True)[:top]\n",
    "\n",
    "    def similarity(self, vect1, vect2):\n",
    "        x = vect1/(np.sqrt(np.sum(vect1**2)) + 1e-8)\n",
    "        y = vect2/(np.sqrt(np.sum(vect2**2)) + 1e-8)\n",
    "        return np.dot(x,y)\n",
    "\n",
    "    def get_negative_sample(self, mini_batch_size, sample_size, word_id, target, corpus, replace=False):\n",
    "#         keys = list(word_id.keys())\n",
    "        container = []\n",
    "        append = container.append\n",
    "        for k in target:\n",
    "            ls = [corpus.count(i) if i != k else 0 for i in word_id.values()]\n",
    "            total = sum(ls)\n",
    "            new = [i/total for i in ls]\n",
    "            new_p = np.power(new, 0.75)\n",
    "            new_p /= np.sum(new_p)\n",
    "            negative = np.random.choice(list(word_id.values()), p=new_p, size=sample_size, replace=False) \n",
    "            append(negative)\n",
    "        \n",
    "        return np.array(container).reshape(len(container), sample_size)\n",
    "    \n",
    "preprocessed = Preprocess('you say goodbye and I say hello.', ',', '.', '\\n')\n",
    "word_id, id_word, corpus = preprocessed.get_word_id()\n",
    "context, target = preprocessed.create_context_target(corpus)\n",
    "# context_onehot, target_onehot = preprocessed.convert_onehot(context, target, len(word_id))\n",
    "negative_sample = preprocessed.get_negative_sample(3,2,word_id, target, corpus)\n",
    "model = CBow(len(word_id), 4, 2, negative_sample)\n",
    "# model = SimpleCbow(len(word_id), 5)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(context, target, max_epoch=10,batch_size=2)\n",
    "# trainer.plot()\n",
    "# cooccurrence_matrix = np.unpackbits(np.load('cooccurrence_matrix3.npy'), axis = 1)\n",
    "\n",
    "#     def PPMI(co_matrix, corpus, verbose=True):\n",
    "#     ppmi_matrix = np.zeros_like(co_matrix, dtype=np.float32)\n",
    "#     N = np.sum(co_matrix)\n",
    "#     sigle_word = np.sum(co_matrix, axis = 0)\n",
    "#     total = co_matrix.shape[0]*co_matrix.shape[1]\n",
    "#     cols = co_matrix.shape[1]\n",
    "#     cnt = 0\n",
    "#     begin = time()\n",
    "#     for i in range(co_matrix.shape[0]):\n",
    "#         for j in range(co_matrix.shape[1]):\n",
    "#             ppmi = np.log2(co_matrix[i,j]*N/(sigle_word[i]*sigle_word[j]) + 1e-8)\n",
    "#             ppmi_matrix[i,j] = max(0, ppmi)\n",
    "#             if verbose:\n",
    "#                 cnt += 1\n",
    "#                 if cnt % (total//200) == 0:\n",
    "#                     print_result(cnt+1,total, begin, time())\n",
    "#     return ppmi_matrix\n",
    "# ppmi = np.load('ppmi.npy')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3501167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T22:24:55.065455Z",
     "start_time": "2023-03-07T22:24:55.065455Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a34e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T22:24:55.065455Z",
     "start_time": "2023-03-07T22:24:55.065455Z"
    }
   },
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467038ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T21:27:46.934305Z",
     "start_time": "2023-03-07T21:27:46.902614Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed27f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T21:27:21.652296Z",
     "start_time": "2023-03-07T21:27:21.637728Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d999a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
