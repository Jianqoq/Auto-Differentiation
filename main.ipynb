{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37cb3cf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T01:18:55.027732Z",
     "start_time": "2023-03-06T01:18:54.065191Z"
    }
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from layers2 import MatMul, softmax\n",
    "from model import SimpleCbow, CBow\n",
    "import numpy as np\n",
    "\n",
    "def print_result(current, total, begin, end):\n",
    "    lis = ['[' if i == 0 else ']' if i == 21 else ' ' for i in range(22)]\n",
    "    index = int(current/total*20)\n",
    "    percentage = format(current*100 / total, '.2f')\n",
    "    if 0 <= index < 20:\n",
    "        pass\n",
    "    else:\n",
    "        index = 20\n",
    "    if index > 0:\n",
    "        for i in range(1,index+1):\n",
    "            lis[i] = u'\\\\u25A0'\n",
    "        string = ''.join(lis)\n",
    "        time = end-  begin\n",
    "        print(f'\\\\r{string} {percentage}% Time: {time:.3f}s', end='', flush=True)\n",
    "    else:\n",
    "        string = ''.join(lis)\n",
    "        time = end-  begin\n",
    "        print(f'\\\\r{string} {percentage}% Time: {time:.3f}s', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8175d0c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T01:18:55.535823Z",
     "start_time": "2023-03-06T01:18:55.029222Z"
    },
    "code_folding": [
     0,
     27,
     95
    ]
   },
   "outputs": [],
   "source": [
    "def remove_duplicate(params, grads):\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j] \n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and params[i].T.shape == params[j].shape and cp.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self,\n",
    "            x,\n",
    "            t,\n",
    "            max_epoch=10,\n",
    "            batch_size=32,\n",
    "            max_grad=None,\n",
    "            eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # シャッフル\n",
    "            idx = cp.random.permutation(cp.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters * batch_size:(iters + 1) * batch_size]\n",
    "                batch_t = t[iters * batch_size:(iters + 1) * batch_size]\n",
    "\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params,\n",
    "                                                 model.grads)  # 共有された重みを1つに集約\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 評価\n",
    "                if (eval_interval\n",
    "                        is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print(\n",
    "                        '\\r| epoch %-5d |  iter %-5d / %-5d | time %10d[s] | loss %.2f'\n",
    "                        % (self.current_epoch + 1, iters + 1, max_iters,\n",
    "                           elapsed_time, avg_loss),\n",
    "                        end='',\n",
    "                        flush=True)\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = cp.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x.get(), self.loss_list, label='train')\n",
    "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(cp.zeros_like(param))\n",
    "                self.v.append(cp.zeros_like(param))\n",
    "\n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * cp.sqrt(1.0 - self.beta2**self.iter) / (\n",
    "            1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "\n",
    "            params[i] -= lr_t * self.m[i] / (cp.sqrt(self.v[i]) + 1e-7)\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, text: str, *args):\n",
    "        dictionary = {i: f' {i}' for i in args}\n",
    "        text = text.lower()\n",
    "        for i in dictionary:\n",
    "            text = text.replace(i, dictionary.get(i))\n",
    "        self.text = text.split(' ')\n",
    "        self.repeated = []\n",
    "        \n",
    "    def get_word_id(self):\n",
    "        dictionary = {}\n",
    "        dictionary2 = {}\n",
    "        corpus = []\n",
    "        append = corpus.append\n",
    "        counter = 0\n",
    "        for index, i in enumerate(self.text):\n",
    "            if i not in dictionary:\n",
    "                dictionary[i] = counter\n",
    "                dictionary2[counter] = i\n",
    "                counter += 1\n",
    "                append(dictionary[i])\n",
    "            else:\n",
    "                append(dictionary[i])\n",
    "                self.repeated.append(index)\n",
    "        return dictionary, dictionary2, corpus\n",
    "\n",
    "    def get_single_context(self,id_word:dict, word_id:dict, corpus: list, word: str,window: int):  # list bound check\n",
    "        text = self.text\n",
    "        word = word.lower()\n",
    "        length = len(text)\n",
    "        if word not in text:\n",
    "            return\n",
    "        ls = [0] * len(corpus)\n",
    "        for index, i in enumerate(text):\n",
    "            if word_id[i] == word_id[word]:    \n",
    "                if index == 0:\n",
    "                    counter = 1\n",
    "                    for k in range(window):\n",
    "                        ls[counter] += 1\n",
    "                        counter += 1\n",
    "                elif index == length - 1:\n",
    "                    counter = 1\n",
    "                    for p in range(window):\n",
    "                        ls[-1-counter] += 1\n",
    "                        counter += 1\n",
    "                else:\n",
    "                    counter = counter2 = 1\n",
    "                    word1_id = word_id[text[index - counter]]\n",
    "                    word2_id = word_id[text[index + counter2]]\n",
    "                    for p in range(window):\n",
    "                        ls[word1_id] += 1\n",
    "                        ls[word2_id] += 1\n",
    "                        counter += 1\n",
    "                        counter2 += 1\n",
    "                        \n",
    "        return cp.array(ls, dtype = 'uint8')\n",
    "\n",
    "    def get_coocurrenceMatrix(self,corpus: list,id_word: dict, word_id: dict, window:int):\n",
    "        ls = []\n",
    "        append = ls.append\n",
    "        total = len(word_id)\n",
    "        begin = time()\n",
    "        for index, i in enumerate(word_id):\n",
    "            append(self.get_single_context(id_word, word_id, corpus, i, window))\n",
    "            print_result(index+1, total, begin, time())\n",
    "        return cp.array(ls, dtype = 'uint8'), ls\n",
    "    \n",
    "    def create_context_target(self, corpus, windowsize = 1):\n",
    "        target = corpus[1 : -1]\n",
    "        context = []\n",
    "        cs = []\n",
    "        cs_append = cs.append\n",
    "        context_append = context.append\n",
    "        for i in range(windowsize, len(corpus)-1):\n",
    "            cs.append(corpus[i-1])\n",
    "            cs.append(corpus[i+1])\n",
    "            context.append(cs)\n",
    "            cs=[]\n",
    "        return cp.array(context), cp.array(target)\n",
    "    \n",
    "    def convert_onehot(self, context, target, length):\n",
    "        zero_context = cp.zeros(shape=(*context.shape, length), dtype = 'uint8')\n",
    "        zero_target = cp.zeros(shape=(*target.shape, length), dtype = 'uint8')\n",
    "        for index, i in enumerate(context):\n",
    "            for index2, k in enumerate(i):\n",
    "                zero_context[index, index2, k] = 1\n",
    "        for index, i in enumerate(target):\n",
    "                zero_target[index, i] = 1\n",
    "        return zero_context, zero_target\n",
    "    \n",
    "    def PPMI(self, co_matrix, corpus, verbose=True):\n",
    "        ppmi_matrix = cp.zeros_like(co_matrix, dtype=cp.float32)\n",
    "        N = cp.sum(co_matrix)\n",
    "        sigle_word = cp.sum(co_matrix, axis = 0)\n",
    "        total = co_matrix.shape[0]*co_matrix.shape[1]\n",
    "        cols = co_matrix.shape[1]\n",
    "        cnt = 0\n",
    "        begin = time()\n",
    "        for i in range(co_matrix.shape[0]):\n",
    "            for j in range(co_matrix.shape[1]):\n",
    "                ppmi = cp.log2(co_matrix[i,j]*N/(sigle_word[i]*sigle_word[j]) + 1e-8)\n",
    "                ppmi_matrix[i,j] = max(0, ppmi)\n",
    "                if verbose:\n",
    "                    cnt += 1\n",
    "                    if cnt % (total//200) == 0:\n",
    "                        print_result(cnt+1,total, begin, time())\n",
    "        return ppmi_matrix\n",
    "    def most_similar(self, matrix:list, word:str,word_id:dict, top:int):\n",
    "        word = word.lower()\n",
    "        if word not in word_id:\n",
    "            return\n",
    "        word_use_vector = matrix[word_id[word]]\n",
    "        ls = {id_word[index]:similarity(word_use_vector, i) for index, i in enumerate(matrix) if index is not word_id[word]}\n",
    "        return sorted(ls.items(),key=lambda x:x[1],reverse=True)[:top]\n",
    "\n",
    "    def similarity(self, vect1, vect2):\n",
    "        x = vect1/(cp.sqrt(cp.sum(vect1**2)) + 1e-8)\n",
    "        y = vect2/(cp.sqrt(cp.sum(vect2**2)) + 1e-8)\n",
    "        return cp.dot(x,y)\n",
    "\n",
    "    def get_negative_sample(self, mini_batch_size, sample_size, word_id, target, corpus, replace=False):\n",
    "#         keys = list(word_id.keys())\n",
    "        container = []\n",
    "        append = container.append\n",
    "        for k in target:\n",
    "            ls = [corpus.count(i) if i != k else 0 for i in word_id.values()]\n",
    "            total = sum(ls)\n",
    "            new = [i/total for i in ls]\n",
    "            new_p = np.power(new, 0.75)\n",
    "            new_p /= np.sum(new_p)\n",
    "            negative = np.random.choice(list(word_id.values()), p=new_p, size=sample_size, replace=False) \n",
    "            append(negative)\n",
    "        \n",
    "        return np.array(container).reshape(mini_batch_size, sample_size)\n",
    "    \n",
    "with open('untitled.txt', mode=\"r\") as fp:\n",
    "         string = fp.read()\n",
    "preprocessed = Preprocess('you say goodbye and I say hello.', ',', '.', '\\n')\n",
    "word_id, id_word, corpus = preprocessed.get_word_id()\n",
    "context, target = preprocessed.create_context_target(corpus)\n",
    "context_onehot, target_onehot = preprocessed.convert_onehot(context, target, len(word_id))\n",
    "sample = preprocessed.get_negative_sample(3,2,word_id, np.array([1,2,3]), corpus)\n",
    "\n",
    "# model = SimpleCbow(len(word_id), 5)\n",
    "# optimizer = Adam()\n",
    "# model.forward(context_onehot, target_onehot)\n",
    "# trainer = Trainer(model, optimizer)\n",
    "# trainer.fit(context_onehot, target_onehot, 100, 50)\n",
    "# trainer.plot()\n",
    "\n",
    "# cooccurrence_matrix = cp.ucpackbits(cp.load('cooccurrence_matrix3.cpy'), axis = 1)\n",
    "\n",
    "#     def PPMI(co_matrix, corpus, verbose=True):\n",
    "#     ppmi_matrix = cp.zeros_like(co_matrix, dtype=cp.float32)\n",
    "#     N = cp.sum(co_matrix)\n",
    "#     sigle_word = cp.sum(co_matrix, axis = 0)\n",
    "#     total = co_matrix.shape[0]*co_matrix.shape[1]\n",
    "#     cols = co_matrix.shape[1]\n",
    "#     cnt = 0\n",
    "#     begin = time()\n",
    "#     for i in range(co_matrix.shape[0]):\n",
    "#         for j in range(co_matrix.shape[1]):\n",
    "#             ppmi = cp.log2(co_matrix[i,j]*N/(sigle_word[i]*sigle_word[j]) + 1e-8)\n",
    "#             ppmi_matrix[i,j] = max(0, ppmi)\n",
    "#             if verbose:\n",
    "#                 cnt += 1\n",
    "#                 if cnt % (total//200) == 0:\n",
    "#                     print_result(cnt+1,total, begin, time())\n",
    "#     return ppmi_matrix\n",
    "# ppmi = cp.load('ppmi.cpy')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3501167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T01:18:58.060879Z",
     "start_time": "2023-03-06T01:18:55.536816Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50915264 0.43929786 0.50718813 0.51648316] [ 0.50040126 -3.3348393   0.39297873  0.90140792]\n",
      "[0.50012089 0.51964791 0.48462941 0.4605512 ] [ 0.00660858  1.07464129 -0.84052545 -2.16103014]\n",
      "[0.4820089  0.51046305 0.51006109 0.49598656] [-0.9839404   0.57206472  0.55008186 -0.21940634]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(1.98775283)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CBow(len(word_id), 4, 2, [1,2,3])\n",
    "model.forward(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90a34e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T01:18:58.076276Z",
     "start_time": "2023-03-06T01:18:58.062370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467038ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
