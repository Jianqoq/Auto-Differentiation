{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "37cb3cf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:45:58.598978Z",
     "start_time": "2023-03-08T06:45:58.584980Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba as nb\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from layers2 import MatMul, softmax\n",
    "from model import SimpleCbow, CBow\n",
    "\n",
    "def print_result(current, total, begin):\n",
    "    lis = ['[' if i == 0 else ']' if i == 21 else ' ' for i in range(22)]\n",
    "    index = int((current+1)/total*20)\n",
    "    percentage = format((current+1)*100 / total, '.2f')\n",
    "    if 0 <= index < 20:\n",
    "        pass\n",
    "    else:\n",
    "        index = 20\n",
    "    if index > 0:\n",
    "        for i in range(1,index+1):\n",
    "            lis[i] = u'\\u25A0'\n",
    "        string = ''.join(lis)\n",
    "        try:\n",
    "            time1 = time.time() -  begin\n",
    "        except:\n",
    "            time1 = time() -  begin\n",
    "        print(f'\\r{string} {percentage}% Time: {time1:.3f}s', end='', flush=True)\n",
    "    else:\n",
    "        string = ''.join(lis)\n",
    "        try:\n",
    "            time1 = time.time() -  begin\n",
    "        except:\n",
    "            time1 = time() -  begin\n",
    "        print(f'\\r{string} {percentage}% Time: {time1:.3f}s', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8175d0c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:48:04.638091Z",
     "start_time": "2023-03-08T06:48:04.446594Z"
    },
    "code_folding": [
     98
    ]
   },
   "outputs": [],
   "source": [
    "def remove_duplicate(params, grads):\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and params[\n",
    "                        i].T.shape == params[j].shape and np.all(\n",
    "                            params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self,\n",
    "            x,\n",
    "            t,\n",
    "            *args,\n",
    "            max_epoch=10,\n",
    "            batch_size=32,\n",
    "            max_grad=None,\n",
    "            eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # シャッフル\n",
    "            idx = np.random.permutation(np.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters * batch_size:(iters + 1) * batch_size]\n",
    "                batch_t = t[iters * batch_size:(iters + 1) * batch_size]\n",
    "                if args is not None:\n",
    "                    batch_q = args[0][iters * batch_size:(iters + 1) * batch_size]\n",
    "                    loss = model.forward(batch_x, batch_t, batch_q)\n",
    "                else:\n",
    "                    loss = model.forward(batch_x, batch_t, batch_size)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.weights,\n",
    "                                                 model.grads)\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "                if (eval_interval\n",
    "                        is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print(\n",
    "                        '\\r| epoch %-5d |  iter %-5d / %-5d | time %10d[s] | loss %.2f'\n",
    "                        % (self.current_epoch + 1, iters + 1, max_iters,\n",
    "                           elapsed_time, avg_loss),\n",
    "                        end='',\n",
    "                        flush=True)\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = np.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (\n",
    "            1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "\n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, text: str, *args):\n",
    "        dictionary = {i: f' {i}' for i in args}\n",
    "        text = text.lower()\n",
    "        for i in dictionary:\n",
    "            text = text.replace(i, dictionary.get(i))\n",
    "        self.text = text.split(' ')\n",
    "        self.repeated = []\n",
    "\n",
    "    def get_word_id(self):\n",
    "        dictionary = {}\n",
    "        dictionary2 = {}\n",
    "        corpus = []\n",
    "        append = corpus.append\n",
    "        counter = 0\n",
    "        for index, i in enumerate(self.text):\n",
    "            if i not in dictionary:\n",
    "                dictionary[i] = counter\n",
    "                dictionary2[counter] = i\n",
    "                counter += 1\n",
    "                append(dictionary[i])\n",
    "            else:\n",
    "                append(dictionary[i])\n",
    "                self.repeated.append(index)\n",
    "        return dictionary, dictionary2, corpus\n",
    "\n",
    "    def get_single_context(self, id_word: dict, word_id: dict, corpus: list,\n",
    "                           word: str, window: int):  # list bound check\n",
    "        text = self.text\n",
    "        word = word.lower()\n",
    "        length = len(text)\n",
    "        if word not in text:\n",
    "            return\n",
    "        ls = [0] * len(corpus)\n",
    "        for index, i in enumerate(text):\n",
    "            if word_id[i] == word_id[word]:\n",
    "                if index == 0:\n",
    "                    counter = 1\n",
    "                    for k in range(window):\n",
    "                        ls[counter] += 1\n",
    "                        counter += 1\n",
    "                elif index == length - 1:\n",
    "                    counter = 1\n",
    "                    for p in range(window):\n",
    "                        ls[-1 - counter] += 1\n",
    "                        counter += 1\n",
    "                else:\n",
    "                    counter = counter2 = 1\n",
    "                    word1_id = word_id[text[index - counter]]\n",
    "                    word2_id = word_id[text[index + counter2]]\n",
    "                    for p in range(window):\n",
    "                        ls[word1_id] += 1\n",
    "                        ls[word2_id] += 1\n",
    "                        counter += 1\n",
    "                        counter2 += 1\n",
    "\n",
    "        return np.array(ls, dtype='uint8')\n",
    "\n",
    "    def get_coocurrenceMatrix(self, corpus: list, id_word: dict, word_id: dict,\n",
    "                              window: int):\n",
    "        ls = []\n",
    "        append = ls.append\n",
    "        total = len(word_id)\n",
    "        begin = time()\n",
    "        for index, i in enumerate(word_id):\n",
    "            append(self.get_single_context(id_word, word_id, corpus, i,\n",
    "                                           window))\n",
    "            print_result(index + 1, total, begin, time())\n",
    "        return np.array(ls, dtype='uint8'), ls\n",
    "    \n",
    "    def PPMI(co_matrix, corpus, verbose=True):\n",
    "        ppmi_matrix = np.zeros_like(co_matrix, dtype=np.float32)\n",
    "        N = np.sum(co_matrix)\n",
    "        sigle_word = np.sum(co_matrix, axis = 0)\n",
    "        total = co_matrix.shape[0]*co_matrix.shape[1]\n",
    "        cols = co_matrix.shape[1]\n",
    "        cnt = 0\n",
    "        begin = time()\n",
    "        for i in range(co_matrix.shape[0]):\n",
    "            for j in range(co_matrix.shape[1]):\n",
    "                ppmi = np.log2(co_matrix[i,j]*N/(sigle_word[i]*sigle_word[j]) + 1e-8)\n",
    "                ppmi_matrix[i,j] = max(0, ppmi)\n",
    "                if verbose:\n",
    "                    cnt += 1\n",
    "                    if cnt % (total//200) == 0:\n",
    "                        print_result(cnt+1,total, begin, time())\n",
    "        return ppmi_matrix\n",
    "    \n",
    "    def create_context_target(self, corpus, windowsize=1):\n",
    "        target = corpus[1:-1]\n",
    "        context = []\n",
    "        cs = []\n",
    "        cs_append = cs.append\n",
    "        context_append = context.append\n",
    "        for i in range(windowsize, len(corpus) - 1):\n",
    "            cs.append(corpus[i - 1])\n",
    "            cs.append(corpus[i + 1])\n",
    "            context.append(cs)\n",
    "            cs = []\n",
    "        return np.array(context), np.array(target)\n",
    "\n",
    "    def convert_onehot(self, context, target, length):\n",
    "        zero_context = np.zeros(shape=(*context.shape, length), dtype='uint8')\n",
    "        zero_target = np.zeros(shape=(*target.shape, length), dtype='uint8')\n",
    "        for index, i in enumerate(context):\n",
    "            for index2, k in enumerate(i):\n",
    "                zero_context[index, index2, k] = 1\n",
    "        for index, i in enumerate(target):\n",
    "            zero_target[index, i] = 1\n",
    "        return zero_context, zero_target\n",
    "\n",
    "    def PPMI(self, co_matrix, corpus, verbose=True):\n",
    "        ppmi_matrix = np.zeros_like(co_matrix, dtype=np.float32)\n",
    "        N = np.sum(co_matrix)\n",
    "        sigle_word = np.sum(co_matrix, axis=0)\n",
    "        total = co_matrix.shape[0] * co_matrix.shape[1]\n",
    "        cols = co_matrix.shape[1]\n",
    "        cnt = 0\n",
    "        begin = time()\n",
    "        for i in range(co_matrix.shape[0]):\n",
    "            for j in range(co_matrix.shape[1]):\n",
    "                ppmi = np.log2(co_matrix[i, j] * N /\n",
    "                               (sigle_word[i] * sigle_word[j]) + 1e-8)\n",
    "                ppmi_matrix[i, j] = max(0, ppmi)\n",
    "                if verbose:\n",
    "                    cnt += 1\n",
    "                    if cnt % (total // 200) == 0:\n",
    "                        print_result(cnt + 1, total, begin, time())\n",
    "        return ppmi_matrix\n",
    "\n",
    "    def most_similar(self, matrix: list, word: str, word_id: dict, top: int):\n",
    "        word = word.lower()\n",
    "        if word not in word_id:\n",
    "            return\n",
    "        word_use_vector = matrix[word_id[word]]\n",
    "        ls = {\n",
    "            id_word[index]: self.similarity(word_use_vector, i)\n",
    "            for index, i in enumerate(matrix) if index is not word_id[word]\n",
    "        }\n",
    "        return sorted(ls.items(), key=lambda x: x[1], reverse=True)[:top]\n",
    "\n",
    "    def similarity(self, vect1, vect2):\n",
    "        x = vect1 / (np.sqrt(np.sum(vect1**2)) + 1e-8)\n",
    "        y = vect2 / (np.sqrt(np.sum(vect2**2)) + 1e-8)\n",
    "        return np.dot(x, y)\n",
    "    \n",
    "    def get_negative_sample(self,\n",
    "                            mini_batch_size,\n",
    "                            sample_size,\n",
    "                            word_id,\n",
    "                            target,\n",
    "                            corpus,\n",
    "                            replace=False):\n",
    "        #         keys = list(word_id.keys())\n",
    "        container = []\n",
    "        append = container.append\n",
    "        length = len(target)\n",
    "        begin = time.time()\n",
    "        values = word_id.values()\n",
    "        ls_value = list(values)\n",
    "        for index, k in enumerate(target):\n",
    "            print_result(index, length, begin)\n",
    "            ls = [corpus.count(i) if i != k else 0 for i in values]\n",
    "            total = sum(ls)\n",
    "            new = [i / total for i in ls]\n",
    "            new_p = np.power(new, 0.75)\n",
    "            new_p /= sum(new_p)\n",
    "            negative = np.random.choice(ls_value,\n",
    "                                        p=new_p,\n",
    "                                        size=sample_size,\n",
    "                                        replace=False)\n",
    "            append(negative)\n",
    "            \n",
    "            \n",
    "        return np.array(container).reshape(len(container), sample_size)\n",
    "\n",
    "with open(\"untitled.txt\") as fp:\n",
    "    string = fp.read()\n",
    "preprocessed = Preprocess(string, ',', '.', '\\n')\n",
    "word_id, id_word, corpus = preprocessed.get_word_id()\n",
    "context, target = preprocessed.create_context_target(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3d999a9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:58:40.659514Z",
     "start_time": "2023-03-08T06:48:11.789342Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                    ] 1.82% Time: 627.967s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20748\\1253134771.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# context_onehot, target_onehot = preprocessed.convert_onehot(context, target, len(word_id))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnegative_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_negative_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20748\\1297105966.py\u001b[0m in \u001b[0;36mget_negative_sample\u001b[1;34m(self, mini_batch_size, sample_size, word_id, target, corpus, replace)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mprint_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20748\\1297105966.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mprint_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# context_onehot, target_onehot = preprocessed.convert_onehot(context, target, len(word_id))\n",
    "negative_sample = preprocessed.get_negative_sample(3, 5, word_id, target, corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6dd69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:40:30.331408Z",
     "start_time": "2023-03-08T06:40:30.331408Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a09cd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:47:10.675605Z",
     "start_time": "2023-03-08T06:47:10.675105Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('negative_sample2.npy', negative_sample.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b684817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:40:30.333407Z",
     "start_time": "2023-03-08T06:40:30.333407Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e39145",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:47:10.677604Z",
     "start_time": "2023-03-08T06:47:10.677604Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CBow(len(word_id), 500, 5, negative_sample)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(context, target, negative_sample, max_epoch=50, batch_size=20)\n",
    "# trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ac191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:40:30.336407Z",
     "start_time": "2023-03-08T06:40:30.336407Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f05010b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:40:30.337907Z",
     "start_time": "2023-03-08T06:40:30.337907Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95f74717",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:48:06.532094Z",
     "start_time": "2023-03-08T06:48:06.525596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40248"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad89fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
