{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa08c816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T06:52:05.387461Z",
     "start_time": "2023-03-12T06:52:05.373461Z"
    }
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from layers.layers import MatMul, softmax, WordEmbed, RNN, TimeAffine, TimeEmbedding, TimeRNN, TimeSoftmaxWithLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "738a9c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T06:52:05.418461Z",
     "start_time": "2023-03-12T06:52:05.389961Z"
    },
    "code_folding": [
     0,
     29,
     97,
     126
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicate(params, grads):\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j] \n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self,\n",
    "            x,\n",
    "            t,\n",
    "            max_epoch=10,\n",
    "            batch_size=32,\n",
    "            max_grad=None,\n",
    "            eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # シャッフル\n",
    "            idx = np.random.permutation(np.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters * batch_size:(iters + 1) * batch_size]\n",
    "                batch_t = t[iters * batch_size:(iters + 1) * batch_size]\n",
    "\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params,\n",
    "                                                 model.grads)  # 共有された重みを1つに集約\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 評価\n",
    "                if (eval_interval\n",
    "                        is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print(\n",
    "                        '\\r| epoch %-5d |  iter %-5d / %-5d | time %10d[s] | loss %.2f'\n",
    "                        % (self.current_epoch + 1, iters + 1, max_iters,\n",
    "                           elapsed_time, avg_loss),\n",
    "                        end='',\n",
    "                        flush=True)\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = np.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x.get(), self.loss_list, label='train')\n",
    "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (\n",
    "            1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "\n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, text: str, *args):\n",
    "        dictionary = {i: f' {i}' for i in args}\n",
    "        text = text.lower()\n",
    "        for i in dictionary:\n",
    "            text = text.replace(i, dictionary.get(i))\n",
    "        self.text = text.split(' ')\n",
    "        self.repeated = []\n",
    "        \n",
    "    def get_word_id(self):\n",
    "        dictionary = {}\n",
    "        dictionary2 = {}\n",
    "        corpus = []\n",
    "        append = corpus.append\n",
    "        counter = 0\n",
    "        for index, i in enumerate(self.text):\n",
    "            if i not in dictionary:\n",
    "                dictionary[i] = counter\n",
    "                dictionary2[counter] = i\n",
    "                counter += 1\n",
    "                append(dictionary[i])\n",
    "            else:\n",
    "                append(dictionary[i])\n",
    "                self.repeated.append(index)\n",
    "        return dictionary, dictionary2, corpus\n",
    "\n",
    "    def get_single_context(self,id_word:dict, word_id:dict, corpus: list, word: str,window: int):  # list bound check\n",
    "        text = self.text\n",
    "        word = word.lower()\n",
    "        length = len(text)\n",
    "        if word not in text:\n",
    "            return\n",
    "        ls = [0] * len(corpus)\n",
    "        for index, i in enumerate(text):\n",
    "            if word_id[i] == word_id[word]:    \n",
    "                if index == 0:\n",
    "                    counter = 1\n",
    "                    for k in range(window):\n",
    "                        ls[counter] += 1\n",
    "                        counter += 1\n",
    "                elif index == length - 1:\n",
    "                    counter = 1\n",
    "                    for p in range(window):\n",
    "                        ls[-1-counter] += 1\n",
    "                        counter += 1\n",
    "                else:\n",
    "                    counter = counter2 = 1\n",
    "                    word1_id = word_id[text[index - counter]]\n",
    "                    word2_id = word_id[text[index + counter2]]\n",
    "                    for p in range(window):\n",
    "                        ls[word1_id] += 1\n",
    "                        ls[word2_id] += 1\n",
    "                        counter += 1\n",
    "                        counter2 += 1\n",
    "                        \n",
    "        return np.array(ls, dtype = 'uint8')\n",
    "\n",
    "    def get_coocurrenceMatrix(self,corpus: list,id_word: dict, word_id: dict, window:int):\n",
    "        ls = []\n",
    "        append = ls.append\n",
    "        total = len(word_id)\n",
    "        begin = time()\n",
    "        for index, i in enumerate(word_id):\n",
    "            append(self.get_single_context(id_word, word_id, corpus, i, window))\n",
    "            print_result(index+1, total, begin, time())\n",
    "        return np.array(ls, dtype = 'uint8'), ls\n",
    "    \n",
    "    def create_context_target(self, corpus, windowsize = 1):\n",
    "        target = corpus[1 : -1]\n",
    "        context = []\n",
    "        cs = []\n",
    "        cs_append = cs.append\n",
    "        context_append = context.append\n",
    "        for i in range(windowsize, len(corpus)-1):\n",
    "            cs.append(corpus[i-1])\n",
    "            cs.append(corpus[i+1])\n",
    "            context.append(cs)\n",
    "            cs=[]\n",
    "        return np.array(context), np.array(target)\n",
    "    \n",
    "    def convert_onehot(self, context, target, length):\n",
    "        zero_context = np.zeros(shape=(*context.shape, length), dtype = 'uint8')\n",
    "        zero_target = np.zeros(shape=(*target.shape, length), dtype = 'uint8')\n",
    "        for index, i in enumerate(context):\n",
    "            for index2, k in enumerate(i):\n",
    "                zero_context[index, index2, k] = 1\n",
    "        for index, i in enumerate(target):\n",
    "                zero_target[index, i] = 1\n",
    "        return zero_context, zero_target\n",
    "    \n",
    "    def PPMI(self, co_matrix, corpus, verbose=True):\n",
    "        ppmi_matrix = np.zeros_like(co_matrix, dtype=np.float32)\n",
    "        N = np.sum(co_matrix)\n",
    "        sigle_word = np.sum(co_matrix, axis = 0)\n",
    "        total = co_matrix.shape[0]*co_matrix.shape[1]\n",
    "        cols = co_matrix.shape[1]\n",
    "        cnt = 0\n",
    "        begin = time()\n",
    "        for i in range(co_matrix.shape[0]):\n",
    "            for j in range(co_matrix.shape[1]):\n",
    "                ppmi = np.log2(co_matrix[i,j]*N/(sigle_word[i]*sigle_word[j]) + 1e-8)\n",
    "                ppmi_matrix[i,j] = max(0, ppmi)\n",
    "                if verbose:\n",
    "                    cnt += 1\n",
    "                    if cnt % (total//200) == 0:\n",
    "                        print_result(cnt+1,total, begin, time())\n",
    "        return ppmi_matrix\n",
    "\n",
    "    def most_similar(self, matrix:list, word:str,word_id:dict, top:int):\n",
    "        word = word.lower()\n",
    "        if word not in word_id:\n",
    "            return\n",
    "        word_use_vector = matrix[word_id[word]]\n",
    "        ls = {id_word[index]:similarity(word_use_vector, i) for index, i in enumerate(matrix) if index is not word_id[word]}\n",
    "        return sorted(ls.items(),key=lambda x:x[1],reverse=True)[:top]\n",
    "\n",
    "    def similarity(self, vect1, vect2):\n",
    "        x = vect1/(np.sqrt(np.sum(vect1**2)) + 1e-8)\n",
    "        y = vect2/(np.sqrt(np.sum(vect2**2)) + 1e-8)\n",
    "        return np.dot(x,y)\n",
    "\n",
    "\n",
    "class MatMul:\n",
    "\n",
    "    def __init__(self, W):\n",
    "        self.weights = [W]\n",
    "        self.X = None\n",
    "        self.gradients = [cp.zeros_like(W)]\n",
    "\n",
    "    def forward(self, forward_input):\n",
    "        W, = self.weights\n",
    "        output = cp.dot(forward_input, W)\n",
    "        self.X = forward_input\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_backward_input):\n",
    "        # get weights and calculate dX\n",
    "        W = self.weights[0]\n",
    "        dX = cp.dot(d_backward_input, W.T)\n",
    "\n",
    "        # use stored input to and dinput to calculate dW and store to self.gradients list\n",
    "        dW = cp.dot(self.X.T, d_backward_input)\n",
    "        self.gradients[0][...] = dW\n",
    "\n",
    "        return dX\n",
    "class timeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        self.ignore_label = -1\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "\n",
    "        if ts.ndim == 3:  # 教師ラベルがone-hotベクトルの場合\n",
    "            ts = ts.argmax(axis=2)\n",
    "\n",
    "        mask = (ts != self.ignore_label)\n",
    "\n",
    "        # バッチ分と時系列分をまとめる（reshape）\n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "        mask = mask.reshape(N * T)\n",
    "\n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N * T), ts])\n",
    "        ls *= mask  # ignore_labelに該当するデータは損失を0にする\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= mask.sum()\n",
    "\n",
    "        self.cache = (ts, ys, mask, (N, T, V))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87890abc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T06:52:05.573462Z",
     "start_time": "2023-03-12T06:52:05.419461Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "with open('untitled.txt', mode=\"r\") as fp:\n",
    "         string = fp.read()\n",
    "preprocessed = Preprocess(string, ',', '.')\n",
    "word_id, id_word, corpus = preprocessed.get_word_id()\n",
    "context, target = preprocessed.create_context_target(corpus)\n",
    "context_onehot, target_onehot = preprocessed.convert_onehot(context, target, len(word_id))\n",
    "rn = cp.random.randn\n",
    "V, D, H = 1000, 5, 8\n",
    "embed_w = rn(V, D)\n",
    "rnn_wx = rn(D, H)\n",
    "rnn_wh = rn(H, H)\n",
    "rnn_b = cp.zeros(H)\n",
    "affine_w = rn(H, V)\n",
    "affine_b = cp.zeros(V)\n",
    "x = cp.array(corpus[:2]).reshape(1, 2)\n",
    "y = cp.array(corpus[3:5]).reshape(1, 2)\n",
    "array = cp.array([x, y]).reshape(2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26b40d95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T06:52:05.588961Z",
     "start_time": "2023-03-12T06:52:05.575462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(9.29889034)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward\n",
    "xs = TimeEmbedding(embed_w, 2).forward(array)\n",
    "xs = TimeRNN(rnn_wx, rnn_wh, rnn_b).forward(xs)\n",
    "xs = TimeAffine(affine_w, affine_b).forward(xs)\n",
    "\n",
    "target = cp.array(corpus[1:2+1]).reshape(1, 2)\n",
    "target2 = cp.array(corpus[3+1:5+1]).reshape(1, 2)\n",
    "target = cp.array([target, target2]).reshape(2, 2)\n",
    "loss = TimeSoftmaxWithLoss(2).forward(xs, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddcde10a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T06:52:05.604462Z",
     "start_time": "2023-03-12T06:52:05.590462Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2 3]\n",
      "  [3 4 5]]\n",
      "\n",
      " [[5 6 7]\n",
      "  [7 8 9]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[1,2,3],[3,4,5]],[[5,6,7],[7,8,9]]])\n",
    "print(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "238737b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T06:52:05.619961Z",
     "start_time": "2023-03-12T06:52:05.605961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 1 1 1]\n",
      " [3 4 5 1 1 1]\n",
      " [5 6 7 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([[1, 2, 3, 1, 1, 1],[3, 4, 5, 1, 1, 1],[5, 6, 7, 1, 1, 1]])\n",
    "print(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed9054f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T06:52:05.635462Z",
     "start_time": "2023-03-12T06:52:05.621462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 22,  28,  34,   6,   6,   6],\n",
       "        [ 40,  52,  64,  12,  12,  12]],\n",
       "\n",
       "       [[ 58,  76,  94,  18,  18,  18],\n",
       "        [ 76, 100, 124,  24,  24,  24]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13577957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T06:52:05.650961Z",
     "start_time": "2023-03-12T06:52:05.636961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5826ca6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T06:52:05.666462Z",
     "start_time": "2023-03-12T06:52:05.652462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb2befef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T06:52:05.681961Z",
     "start_time": "2023-03-12T06:52:05.668462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x[0,0,:]*y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8fd3e75b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T06:52:05.697461Z",
     "start_time": "2023-03-12T06:52:05.683461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x[0,0,:]*y[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 608.844,
   "position": {
    "height": "630.844px",
    "left": "1838px",
    "right": "20px",
    "top": "120px",
    "width": "412px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
